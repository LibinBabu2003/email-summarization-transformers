\documentclass[12pt,a4paper]{article}

% ----------------------------
% Packages
% ----------------------------
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{enumitem}
\usepackage{titlesec}

\setstretch{1.3}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue
}

% ----------------------------
% Formatting
% ----------------------------
\titleformat{\section}{\large\bfseries}{\thesection}{0.8em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.8em}{}

% ----------------------------
% Document
% ----------------------------
\begin{document}

% ----------------------------
% Title Page
% ----------------------------
\begin{titlepage}
    \centering
    \vspace*{1.5cm}

    {\Huge \textbf{Email Summarization using Transformer Models}}\\[1.2cm]

    {\Large\textbf{ AI / Machine Learning Project Report}}\\[1.5cm]

    \textbf{Submitted by}\\[0.3cm]
    {\Large Libin Babu}\\
    DATA SCIENCE INTERN\\[1cm]

    

    \vfill
    {\large \today}
\end{titlepage}

% ----------------------------
% Index / Table of Contents
% ----------------------------
\tableofcontents
\newpage

% ----------------------------
% Introduction
% ----------------------------
\section{Introduction}

Emails are one of the most common communication methods used in organizations. 
In professional environments such as customer support, HR, project management, and operations, emails are often very long and contain a mix of important information and unnecessary content such as greetings, signatures, and disclaimers.

Reading and understanding long emails takes time and effort. 
The goal of this project is to automatically summarize long email content into a short and meaningful version while keeping the most important information intact.

This project uses modern Transformer-based models for summarization and compares them with traditional NLP techniques to clearly understand the improvement provided by deep learning.

% ----------------------------
% Problem Statement
% ----------------------------
\section{Problem Statement}

The main problem addressed in this project is:

\textit{How can long emails be automatically summarized in a way that preserves intent, actions, and key information while reducing reading time?}

Challenges include:
\begin{itemize}
    \item Presence of irrelevant information
    \item Long and unstructured content
    \item Difficulty in identifying key action points
\end{itemize}

% ----------------------------
% Objectives
% ----------------------------
\section{Objectives}

The objectives of this project are:
\begin{itemize}
    \item To build an automated email summarization system
    \item To apply Transformer-based pretrained models for abstractive summarization
    \item To compare abstractive and extractive summarization approaches
    \item To identify important sentences influencing the summary
    \item To deploy the system as a simple web application
\end{itemize}

% ----------------------------
% Overall Workflow
% ----------------------------
\section{Overall Workflow}

The complete workflow of the project is as follows:
\begin{enumerate}
    \item User provides a long email as input
    \item Email is split into sentences and tokens
    \item Transformer model generates an abstractive summary
    \item TF-IDF is used to generate an extractive summary
    \item Important sentences are identified based on similarity
    \item Results are displayed using a web interface
\end{enumerate}

This workflow ensures both performance and explainability.

% ----------------------------
% Transformer-Based Abstractive Summarization
% ----------------------------
\section{Abstractive Summarization using Transformers}

\subsection{What is Abstractive Summarization}

Abstractive summarization generates new sentences that capture the meaning of the original text.
Instead of copying sentences, the model understands the content and rewrites it in a shorter form, similar to how humans summarize.

\subsection{Why Transformers are Used}

Transformers are used because:
\begin{itemize}
    \item They can process the entire email at once
    \item Self-attention helps focus on important parts
    \item They handle long text better than traditional models
\end{itemize}

\subsection{How the Model Works}

The Transformer model first encodes the email into contextual representations.
Using attention mechanisms, it identifies important information.
The decoder then generates a concise summary based on this understanding.

% ----------------------------
% Extractive Summarization using TF-IDF
% ----------------------------
\section{Extractive Summarization using TF-IDF}

\subsection{What is Extractive Summarization}

Extractive summarization selects important sentences directly from the original email without modifying them.
The selected sentences together form the summary.

\subsection{TF-IDF Technique}

TF-IDF (Term Frequency–Inverse Document Frequency) is a classical NLP method used to measure word importance.
Words that are frequent in a sentence but rare across the email are given higher weight.

\subsection{Sentence Selection}

Each sentence is scored using TF-IDF values.
Sentences with the highest scores are selected as part of the extractive summary.
This method is simple and interpretable but does not understand meaning deeply.

% ----------------------------
% Important Sentence Identification
% ----------------------------
\section{Important Sentence Identification}

\subsection{Need for Explainability}

Transformer models are often seen as black boxes.
To improve transparency, it is useful to show which parts of the email influenced the generated summary.

\subsection{Similarity-Based Approach}

Each sentence and the generated summary are converted into numerical vectors.
Cosine similarity is used to measure how close each sentence is to the summary.
Sentences with the highest similarity scores are considered important.

\subsection{Benefits}

This method helps users understand:
\begin{itemize}
    \item Why certain information appears in the summary
    \item Which sentences influenced the model output
\end{itemize}

% ----------------------------
% Implementation Details
% ----------------------------
\section{Implementation Details}

The project is implemented using Python.
Key libraries include:
\begin{itemize}
    \item Transformer library for pretrained models
    \item NLTK for sentence tokenization
    \item Scikit-learn for TF-IDF and similarity computation
    \item Streamlit for building the user interface
\end{itemize}

% ----------------------------
% Deployment
% ----------------------------
\section{Deployment}

The application is deployed online using a free hosting platform.
The deployment allows users to interact with the model in real time by pasting email content and viewing summaries instantly.

% ----------------------------
% Results and Discussion
% ----------------------------
\section{Results and Discussion}

Observations from the project include:
\begin{itemize}
    \item Abstractive summaries are more natural and concise
    \item Extractive summaries are easier to interpret but less fluent
    \item Important sentence visualization improves trust in the model
\end{itemize}

% ----------------------------
% Limitations
% ----------------------------
\section{Limitations}

Some limitations of the current system are:
\begin{itemize}
    \item Possible loss of minor details in abstractive summaries
    \item Risk of hallucinated content
    \item Token length limitations for very long emails
\end{itemize}

% ----------------------------
% Future Scope
% ----------------------------
\section{Future Scope}

Possible future improvements include:
\begin{itemize}
    \item Fine-tuning the model on email-specific datasets
    \item Adding support for multiple languages
    \item Visualizing actual attention weights
    \item Integrating with enterprise email tools
\end{itemize}

% ----------------------------
% Conclusion
% ----------------------------
\section{Conclusion}

This project demonstrates a practical approach to email summarization using Transformer-based models.
By comparing abstractive and extractive techniques and adding explainability, the system provides both accuracy and transparency.
The project reflects real-world AI/ML application rather than just theoretical implementation.

% ----------------------------
% References
% ----------------------------
\section*{References}
\begin{itemize}
    \item Attention Is All You Need – Vaswani et al.
    \item Transformer and NLP documentation
    \item Streamlit documentation
\end{itemize}

\end{document}
